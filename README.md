# ğŸ” Disaggregated KV Caching for LLM Inference (WIP)

## Overview

This project explores **disaggregated KV cache storage** to enable **long-context LLM inference** beyond GPU memory limits. It introduces a tiered caching architecture that offloads **Key/Value tensors** from GPU to CPU (and eventually to disk), while maintaining throughput through asynchronous overlap of compute and I/O.

âš ï¸ **Under active development.** Expect ongoing optimizations, new features, and performance experiments.

---

## âœ¨ Key Features (Planned & In Progress)

* âœ… GPU â†’ CPU offloading of KV cache blocks
* âœ… **Double-buffered async decode** using CUDA streams for overlapping data transfer and attention compute
* ğŸ”„ Three-stage pipeline: **load â†’ compute â†’ merge**
* ğŸš§ KV Block Manager to minimize fragmentation across varying sequence lengths
* ğŸš§ **Log-sum-exp partial attention** across streamed KV blocks
* ğŸ”„ Future: Disk-tier offload for ultra-long contexts (100K+ tokens)
* âš™ï¸ Aim: virtually **unlimited context** on constrained GPUs, with bounded throughput slowdown (<2Ã—)

---

## ğŸ”¬ Architecture Summary

```
[GPU KV Buffer] â†â†’ [Pinned CPU Memory] â†â†’ [Disk Backend (planned)]
       â–²                 â–²
 Memory â”‚           Async â”‚
 Stream â”‚ memcpyAsync   Transfer
       â”‚                 â”‚
 [Attention Engine] â†â†’ [KV Block Manager]
```

* **KVBlockManager:** Manages allocation, eviction, and tier transitions
* **Attention Engine:** Custom kernel leveraging `cudaMemcpyAsync` and double buffers
* **OffloadManager:** Schedules data movement across CUDA streams and CPU memory

---

## âš™ï¸ Tech Stack

* ğŸ§  **Frameworks:** PyTorch, C++ extensions
* ğŸš€ **Parallelism:** CUDA streams, double buffering, asynchronous memory transfers
* ğŸ› ï¸ **Languages:** Python, C++
* ğŸ“¦ **Deployment:** Docker, AWS

---

## ğŸ§ª Benchmarks (Ongoing)

| Context | VRAM Usage | Throughput (tokens/s) | Offload Stage              |
| ------- | ---------- | --------------------- | -------------------------- |
| 8K      | Baseline   | TBD                   | N/A                        |
| 16K     | â€“30%       | TBD                   | GPU â†’ CPU                  |
| 32K+    | TBD        | TBD                   | GPU â†’ CPU â†’ Disk (planned) |

âš¡ **Goal:** Keep throughput slowdown underÂ 2Ã—Â while scaling context length.

---

## ğŸ“Œ Development Roadmap

1. Stabilize **double-buffered async decode** and benchmark against baseline
2. Implement **log-sum-exp accumulation** in streamed attention kernel
3. Add **disk-tier offload** and adaptive caching policies
4. Integrate a demo notebook showcasing 32K+ token inference

---

## ğŸ“¬ Contact & Collaboration

Maintained by **Ishan Revankar**
ğŸ”— [LinkedIn](https://www.linkedin.com/in/ishanrev/)
ğŸ“« Open an issue or reach out for ideas and contributions!

> âš ï¸ Work in progressâ€”expect breaking changes and updates frequently.
